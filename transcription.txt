 Did you know a simple physics equation from 200 years ago is behind the AI images created by models like DAL-E, Imogen, and Mid-Journey? How can a single equation describe the physics of heat, the random zig-zagging of particles, and the astonishing images generated by modern AI? Before getting into the concepts, let's briefly review the journey of the diffusion equation. It has a fascinating history that connects physics to the heart of modern AI image generation. It all started with Fourier in the 19th century, who introduced the equation to model how heat flows through materials. Then came Einstein in the 20th century, who used it to describe Brownian motion, the random zigzagging of tiny particles in a liquid. A few years later, the Langevin equation added the idea of random forces helping us understand stochastic motion. The Falker-Plank equation expanded this framework to calculate probabilities for where particles might be. The diffusion equation reemerged as a special case of the Falker-Plank equation. Building on this foundation, Yasha Soldikstein brought the concept into AI, linking diffusion processes to image generation. For this work, they used other developments in statistical physics by physicists like Jarsinski, which prescribed transitions between equilibrium states, leading to techniques like annealed importance sampling and machine learning. But how does the diffusion model work for AI image generation? Imagine an artist learning to create a perfect sculpture from raw wood. One way they might learn is by taking a masterpiece, undoing all the carvings and reconstructing the raw block of wood. Through this reverse process, they discover the exact steps needed to create something even better than the original masterpiece. Similarly, in AI, the diffusion models start with noise, like a blank canvas, and learn to carve away the randomness to create something beautiful. The result? Stunning AI-generated images that feel like they come from a master artist. But how exactly do we mathematically start from Newton's law of motion for a particle under random force from a background liquid and end up with breathtaking images? How does a deterministic equation like Newton's law turn into a probabilistic description and play a central role in both physics and AI. Are the machine learning diffusion models we use today fully self-consistent? Or do they have limitations? And if they do, how can we refine these models to create even more stunning and realistic images? Let's break this down step by step, starting with the Langevin equation. This equation is Newton's second law, describing a particle released in a background liquid. The equation describes how the particle moves when it's influenced by three forces. Friction, due to the viscosity of the liquid. A predictable force due to, for example, gravity or electric field. And a random force due to collisions with other particles in the fluid. This randomness is what makes the motion stochastic and brings in the idea of explaining the motion of the particle using a probability distribution. Instead of asking where the particle is, we now ask, what is the probability of finding the particle at position x at a given time t? In the absence of a well-defined external force, the probability will take a Gaussian form with a mean of zero and a variance that increases with time. The interpretation is that if we release a particle in a fluid in the absence of forces like gravity, it will move randomly left and right and up and down, such that the average position is always the initial position. But the uncertainty in the position, the variance, will increase with time. To be exact, the variance is two times a constant D multiplied by time. The constant D accounts for the strength of the kicks by other particles and depends on the temperature of the background liquid. Surprisingly, this probability distribution is a solution of the diffusion equation. The surprise is that the diffusion equation was first introduced to explain how heat spreads in a material. As we see now, it also explains how the probability of a particle in a liquid evolves and spreads over the container that contains the liquid. The diffusion equation is, however, a special case of a more general formula called the Fokker-Planck equation. If you remember, we said the diffusion equation describes the probability of our particle in the absence of an external force like gravity. In the presence of such forces, the diffusion equation must be replaced by the Fokker-Planck equation, here is where things get intriguing and a bit tricky. In the standard diffusion process, the probabilities never truly reach equilibrium. And the latter is the assumption for how machine learning diffusion models work. So, for this absence of equilibrium in a diffusion model, we don't have a perfect recipe, as prescribed in statistical physics, that perfectly starts with a complete noise and reverses back to a breathtaking image. We will get back to this at the end of the video. For now, just remember that the diffusion equation of probability is just a special case of the Fokker-Planck equation for when external forces are absent. Let's now get into another important subject, the Markovian property of the diffusion equation. The Markovian property means that the future state of a system depends only on its present state, not on its history. In simpler terms, the system has no memory. It evolves step by step, with each step determined solely by where it is at that moment. For the diffusion equation, this property ensures that the probability of a particle's position at a given time is based entirely on its current position, not on the path it took to get there. In the context of AI image generation, this Markovian property is crucial. It means that the model can systematically add or remove noise one step at a time without needing to account for the entire sequence of steps that came before. We will get back to this later. Before we get into the subject of how the diffusion model is used in the field of machine learning to generate images, we need to cover another result from statistical physics. How to infer the probability of an equilibrium state from the probability of another well-known equilibrium state by summing over all the non-equilibrium works that one can perform on the system. This concept hinges on a powerful mathematical identity, often associated with Jarzynski's equality. The key idea is this. The ratio of probabilities between two equilibrium states can be expressed as an ensemble average over an exponential term involving work where subscripts final and initial refer to the equilibrium probabilities of the final and initial states. W is the work done to move between these states and beta is the inverse of temperature. Now, why does this matter? In practice, this identity gives us a recipe for moving between equilibrium states. Imagine we want to transition a system from one stable configuration to another. We can perform rapid, non-equilibrium transformations. By measuring the work done along these trajectories and applying the exponential weighting, we can compute the ratio of the probabilities of the two states. That means that if we know the probability of one state, we have a recipe for how to gradually convert it to the probability of the other state. This is exactly the recipe we need to convert the complete noise, as one of the two states with a known probability to a nice-looking image with an unknown probability as the other equilibrium state. Let's now discuss how diffusion models work in machine learning and their connection to the concepts we've explored so far. In these models, instead of tracking the position of a particle, we work with the values of image pixels. Interestingly, the diffusion equation doesn't distinguish between particle positions and pixel values. It treats them both as vector x. For machine learning, this vector refers to the values of the pixels at time t. The overall goal is to estimate the probability distribution that describes how likely it is to generate an image similar to the real ones in our dataset. To achieve this, we assume that the images in the dataset are samples drawn from a true, well-defined probability distribution. This serves as our initial equilibrium state. At the other end of the spectrum, we define another equilibrium state, pure noise, where all the structure in the images is destroyed. The training process involves modeling the forward and reverse trajectories between these two states using Jarzinski's equality in statistical physics, which we discussed earlier in the video. Here's how it works. In a process called forward trajectory, we start from the real images and gradually add noise step by step, simulating the forward trajectory from the structured state to the pure noise equilibrium. In another process called the reverse trajectory, we train the model to undo the forward process step by step, removing the noise and reconstructing the structured state. The key is to calculate the joint probability of the entire trajectory for both forward and reverse processes using the Markovian property of diffusion equation. From this we compute the marginal probability of the initial state, in other words, the nice looking image, in two ways. Once using the forward trajectory probability, and once using the reverse trajectory probability. These two marginal probabilities should ideally be identical because they both describe the same system, the state of nice looking images. In practice, we aim to minimize the difference between them, which corresponds to minimizing the loss of information. This minimal loss can be mathematically translated to maximizing the likelihood, and it forms the foundation for training the model using existing images in our dataset. When it's time to generate an image, after the training is done, the process runs in reverse. Starting from pure noise, the second equilibrium state, the model follows the reverse trajectory, gradually removing noise and turning randomness into a coherent, high-quality image. This elegant approach combines physics, probability, and machine learning to bridge the gap between randomness and creativity, enabling AI to produce stunning, lifelike images. Could this same approach apply to other AI models beyond image generation? Interestingly, it already has. In a recent work by Google DeepMind, this same diffusion-based technology was adapted for weather forecasting. By treating weather patterns as dynamic systems evolving over time, just like pixel values or particle positions, the diffusion process can model how these patterns change and predict future states with remarkable accuracy. This opens up a world of possibilities. If diffusion models can create stunning images and predict complex phenomena like the weather, what other areas could they transform? Do you think we can use them to generate realistic videos or even improve natural language models? از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از in science can go beyond their original context to spark entirely new fields. In the particular case that we discussed today, what began as a way to describe the random motion of a particle in a fluid has been transformed into a powerful AI tool capable of generating state-of-the-art images. As the second point, let me mention an issue that, in my opinion, stands out in current machine learning diffusion models. The solution to the diffusion equation, the noise, lacks a true equilibrium state. It continues to evolve indefinitely. In contrast, AI diffusion models rely on a key identity from statistical physics, which assumes transition between two well-defined equilibrium states. از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از این روزید نقیزی از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از ازقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مقسمیزه در مق you